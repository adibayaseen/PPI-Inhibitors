{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adibayaseen/PPI-Inhibitors/blob/main/GNN_based_pipeline_Training_for_Predicting_small_molecule_inhibition_of_protein_complexes_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5zy8pv8_EQa"
      },
      "source": [
        "**Set the Runtime->Change Runtime Type to GPU**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLKfwNHlwnSE"
      },
      "source": [
        "# Protein 3d structure assessment with graph neural networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uOhzY0qAj-0",
        "outputId": "17ca6f29-1a6a-452f-845b-875dbebf46b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_3U6PueCCgv",
        "outputId": "f188ada9-49fb-4f7c-bd12-f0d8a9f3ff15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'PPI-Inhibitors': No such file or directory\n",
            "Collecting biopython\n",
            "  Downloading biopython-1.83-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython) (1.23.5)\n",
            "Installing collected packages: biopython\n",
            "Successfully installed biopython-1.83\n",
            "Cloning into 'PPI-Inhibitors'...\n",
            "remote: Enumerating objects: 1237, done.\u001b[K\n",
            "remote: Counting objects: 100% (432/432), done.\u001b[K\n",
            "remote: Compressing objects: 100% (301/301), done.\u001b[K\n",
            "remote: Total 1237 (delta 164), reused 395 (delta 129), pack-reused 805\u001b[K\n",
            "Receiving objects: 100% (1237/1237), 2.59 GiB | 28.94 MiB/s, done.\n",
            "Resolving deltas: 100% (346/346), done.\n",
            "Updating files: 100% (597/597), done.\n"
          ]
        }
      ],
      "source": [
        "#!rm -r Data\n",
        "!rm -r PPI-Inhibitors\n",
        "!pip install biopython\n",
        "!git clone https://github.com/adibayaseen/PPI-Inhibitors\n",
        "#!pip install py3Dmol"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uE6yIRsQ7C5Y",
        "outputId": "de837a50-85e3-4656-dfe0-a4a4e46a9e1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rdkit\n",
            "  Downloading rdkit-2023.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rdkit) (1.23.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit) (9.4.0)\n",
            "Installing collected packages: rdkit\n",
            "Successfully installed rdkit-2023.9.4\n"
          ]
        }
      ],
      "source": [
        "!pip install rdkit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjV5-TPeGt-0",
        "outputId": "5e30df4a-e185-4942-e5fb-b2faea95ad53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Equivalent epochs in one iteration of data loader 1.1\n",
            "[[('14', '20', '29', '43', '79', '75', '78', '24', '56', '61'), ('-14', '-20', '-29', '-43', '-79', '-75', '-78', '-24', '-56', '-61')], tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0])]\n",
            "[[('4', '10', '33', '45', '49', '31', '53', '18', '23', '61'), ('-4', '-10', '-33', '-45', '-49', '-31', '-53', '-18', '-23', '-61')], tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0])]\n",
            "[[('3', '9', '71', '85', '90', '99', '50', '37', '65', '84'), ('-3', '-9', '-71', '-85', '-90', '-99', '-50', '-37', '-65', '-84')], tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0])]\n",
            "[[('0', '22', '63', '69', '70', '52', '42', '6', '37', '26'), ('0', '-22', '-63', '-69', '-70', '-52', '-42', '-6', '-37', '-26')], tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0])]\n",
            "[[('21', '59', '82', '91', '98', '5', '53', '86', '54', '11'), ('-21', '-59', '-82', '-91', '-98', '-5', '-53', '-86', '-54', '-11')], tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0])]\n",
            "[[('55', '57', '72', '81', '97', '5', '23', '1', '23', '89'), ('-55', '-57', '-72', '-81', '-97', '-5', '-23', '-1', '-23', '-89')], tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0])]\n",
            "[[('13', '38', '48', '64', '80', '56', '66', '18', '11', '1'), ('-13', '-38', '-48', '-64', '-80', '-56', '-66', '-18', '-11', '-1')], tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0])]\n",
            "[[('19', '28', '73', '83', '87', '77', '60', '26', '6', '84'), ('-19', '-28', '-73', '-83', '-87', '-77', '-60', '-26', '-6', '-84')], tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0])]\n",
            "[[('25', '35', '36', '46', '95', '31', '89', '44', '89', '96'), ('-25', '-35', '-36', '-46', '-95', '-31', '-89', '-44', '-89', '-96')], tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0])]\n",
            "[[('8', '12', '27', '32', '40', '65', '89', '50', '94', '1'), ('-8', '-12', '-27', '-32', '-40', '-65', '-89', '-50', '-94', '-1')], tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0])]\n",
            "[[('16', '39', '41', '88', '92', '18', '60', '93', '93', '50'), ('-16', '-39', '-41', '-88', '-92', '-18', '-60', '-93', '-93', '-50')], tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0])]\n",
            "['68', '11', '13', '61', '22', '65', '78', '51', '42', '21', '16', '84', '30', '88', '57', '0', '26', '55', '51', '21', '3', '99', '30', '60', '5', '57', '14', '70', '14', '0', '24', '10', '37', '28', '99', '54', '46', '77', '81', '13', '41', '77', '29', '37', '65', '69', '64', '98', '51', '55', '37', '5', '96', '98', '74', '61', '51', '69', '24', '75', '23', '23', '47', '37', '76', '99', '48', '91', '66', '95', '5', '79', '56', '7', '24', '47', '78', '87', '83', '4', '38', '85', '73', '29', '46', '19', '23', '58', '88', '0', '7', '63', '64', '38', '49', '25', '38', '67', '96', '31']\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Tue Jan  9 16:43:15 2024\n",
        "\n",
        "@author: u1876024\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler\n",
        "from torch.utils.data.sampler import WeightedRandomSampler\n",
        "import numpy as np\n",
        "\n",
        "class BalancedDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A custom dataset class that creates a balanced dataset from imbalanced data.\n",
        "    This dataset calculates sample weights inversely proportional to class frequencies,\n",
        "    which can be used with a WeightedRandomSampler to achieve balanced batches.\n",
        "\n",
        "    NOTE: As it involves stochastic sampling, there is a chance that a few training examples are actually never selected.\n",
        "\n",
        "    Attributes:\n",
        "        data (array-like): The input data. Can be a list, NumPy array, or PyTorch tensor.\n",
        "        labels (array-like): The labels corresponding to the data. Should be a 1D array-like object.\n",
        "        sample_weights (torch.Tensor): Weights for each sample, inversely proportional to class frequencies.\n",
        "\n",
        "    Methods:\n",
        "        __len__: Returns the number of samples in the dataset.\n",
        "        __getitem__(idx): Returns the sample and its corresponding label at the given index.\n",
        "    \"\"\"\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "\n",
        "        # Count the number of examples in each class\n",
        "        class_counts = np.bincount(self.labels)\n",
        "        # Assign weight inversely proportional to class frequency\n",
        "        weights = 1. / torch.tensor(class_counts, dtype=torch.float)\n",
        "        # Create a weight list for each sample\n",
        "        self.sample_weights = weights[labels]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.labels[idx]\n",
        "\n",
        "def create_balanced_loader(data, labels, batch_size=32):\n",
        "    \"\"\"\n",
        "    Creates a DataLoader with balanced batches for a given dataset.\n",
        "    This function is useful for training models on imbalanced datasets.\n",
        "\n",
        "    Args:\n",
        "        data (array-like): The input data. Can be a list, NumPy array, or PyTorch tensor.\n",
        "        labels (array-like): The labels corresponding to the data. Should be a 1D array-like object.\n",
        "        batch_size (int, optional): The size of each batch. Default is 32.\n",
        "\n",
        "    Returns:\n",
        "        DataLoader: A PyTorch DataLoader that yields balanced batches.\n",
        "\n",
        "    Usage Example:\n",
        "        >>> data = [features1, features2, ...]  # Replace with your data features\n",
        "        >>> labels = [label1, label2, ...]     # Replace with your data labels\n",
        "        >>> balanced_loader = create_balanced_loader(data, labels, batch_size=32)\n",
        "        >>> for batch_data, batch_labels in balanced_loader:\n",
        "        >>>     # Train your model using the balanced batches\n",
        "    \"\"\"\n",
        "    dataset = BalancedDataset(data, labels)\n",
        "    # WeightedRandomSampler will take care of the balancing\n",
        "    sampler = WeightedRandomSampler(weights=dataset.sample_weights, num_samples=len(dataset.sample_weights), replacement=True)\n",
        "\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, sampler=sampler)\n",
        "    return loader\n",
        "\n",
        "\n",
        "class BinaryBalancedSampler(Sampler):\n",
        "    \"\"\"\n",
        "    A PyTorch Sampler that returns batches with an equal number of positive and negative examples.\n",
        "    The sampler oversamples from the minority class to balance the majority class, ensuring that\n",
        "    each batch contains 50% positive and 50% negative examples.\n",
        "\n",
        "    NOTE: It leads to more examples in single iteration through the data loader than in one epoch\n",
        "\n",
        "    Attributes:\n",
        "        class_vector (list or numpy array): class labels.\n",
        "        batch_size (int): The size of each batch.\n",
        "        n_splits (int): The number of batches/splits in the dataset.\n",
        "        equivalent_epochs (float): The number of times the sampler goes over the minority class\n",
        "                                   in one complete iteration of the DataLoader.\n",
        "\n",
        "    Methods:\n",
        "        gen_sample_array: Yields indices for each batch ensuring class balance.\n",
        "        __iter__: Returns an iterator over batch indices.\n",
        "        __len__: Returns the number of batches in the sampler.\n",
        "    \"\"\"\n",
        "    def __init__(self, class_vector, batch_size = 10):\n",
        "        \"\"\"\n",
        "        Arguments\n",
        "        ---------\n",
        "        class_vector : torch tensor\n",
        "            a vector of class labels\n",
        "        \"\"\"\n",
        "        self.batch_size = batch_size\n",
        "        self.class_vector = class_vector\n",
        "        YY = np.array(self.class_vector)\n",
        "        U, C = np.unique(YY, return_counts=True)\n",
        "        M = U[np.argmax(C)]        #find majority class\n",
        "        Midx = np.nonzero(YY==M)[0] #indices of majority class\n",
        "        midx = np.nonzero(YY!=M)[0] #indices of minority class\n",
        "        midx_ = np.random.choice(midx,size=len(Midx))     #oversample minority indices so they are equal to majority ones\n",
        "        self.YY = np.array(list(YY[Midx])+list(YY[midx_]))\n",
        "        self.idx = np.array(list(Midx)+list(midx_))\n",
        "        self.n_splits = int(np.ceil(len(self.idx)/self.batch_size))\n",
        "        self.equivalent_epochs = len(self.idx)/len(self.class_vector)\n",
        "        print('Equivalent epochs in one iteration of data loader',self.equivalent_epochs)\n",
        "\n",
        "    def gen_sample_array(self):\n",
        "        from sklearn.model_selection import StratifiedKFold\n",
        "        skf = StratifiedKFold(n_splits= self.n_splits,shuffle=True)\n",
        "        for tridx,ttidx in skf.split(self.idx,self.YY):\n",
        "            yield np.array(self.idx[ttidx])\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter(self.gen_sample_array())\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_splits\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.labels[idx]\n",
        "\n",
        "if __name__=='__main__':\n",
        "\n",
        "    E = [(str(p_i), str(-1*c_i)) for p_i, c_i in zip(range(100), range(100))]  # Replace with your data\n",
        "    Y = np.random.randint(0, 2, size=100)  # Replace with your labels\n",
        "    batch_size = 10\n",
        "\n",
        "    dataset = CustomDataset(E, Y)\n",
        "    batch_sampler = BinaryBalancedSampler(Y, batch_size)\n",
        "    data_loader = DataLoader(dataset, batch_sampler=batch_sampler)\n",
        "\n",
        "    for batch in data_loader:\n",
        "        print(batch)\n",
        "\n",
        "    # Example usage of create_balanced_loader\n",
        "    balanced_loader = create_balanced_loader(E, Y, batch_size)\n",
        "\n",
        "    # Iterate over the DataLoader\n",
        "    L = []\n",
        "    for (pid,cid),label in balanced_loader:\n",
        "        # Process your batches\n",
        "        L.extend(pid)\n",
        "    print(L)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 844
        },
        "id": "hn0xj2yWfAYq",
        "outputId": "4649299c-ec81-4b38-e485-b11f791489a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available. Number of GPUs: 1\n",
            "GPU 0: Tesla T4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15695/15695 [00:00<00:00, 617370.52it/s]\n",
            "<ipython-input-5-aa9bb44951ab>:458: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  Alldata=np.array(Alldata)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Equivalent epochs in one iteration of data loader 1.8467946396233248\n",
            "test complex  1YCQ\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/5 [00:00<?, ?it/s]\n",
            "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "  5%|▌         | 1/20 [00:10<03:16, 10.33s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOADED BEST AUCROC 0.9256965944272445 AUCPR 0.19281742671942217\n",
            "AUCROC 0.9256965944272445 AUCPR 0.19281742671942217 best aucroc 0.9256965944272445\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 10%|█         | 2/20 [00:20<03:01, 10.07s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOADED BEST AUCROC 0.9380804953560371 AUCPR 0.2306377558576972\n",
            "AUCROC 0.9380804953560371 AUCPR 0.2306377558576972 best aucroc 0.9380804953560371\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 15%|█▌        | 3/20 [00:30<02:52, 10.15s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOADED BEST AUCROC 0.9479313256403039 AUCPR 0.25454314520323174\n",
            "AUCROC 0.9479313256403039 AUCPR 0.25454314520323174 best aucroc 0.9479313256403039\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 20%|██        | 4/20 [00:39<02:33,  9.62s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOADED BEST AUCROC 0.9555305375738812 AUCPR 0.28488396539779937\n",
            "AUCROC 0.9555305375738812 AUCPR 0.28488396539779937 best aucroc 0.9555305375738812\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 25%|██▌       | 5/20 [00:48<02:21,  9.41s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOADED BEST AUCROC 0.9575007036307346 AUCPR 0.3250377148104421\n",
            "AUCROC 0.9575007036307346 AUCPR 0.3250377148104421 best aucroc 0.9575007036307346\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▌       | 5/20 [00:58<02:54, 11.66s/it]\n",
            "  0%|          | 0/5 [00:58<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-aa9bb44951ab>\u001b[0m in \u001b[0;36m<cell line: 464>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    588\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_pids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_cids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_labels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m                       \u001b[0mpids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_pids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m                       \u001b[0mG_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mGNN_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAll_ProteinData_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;31m#pass each unique complex through the GNN once\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m                       \u001b[0mGNN_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mG_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#append to make examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m                       \u001b[0;32mdel\u001b[0m \u001b[0mG_dict\u001b[0m \u001b[0;31m#clear up memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-aa9bb44951ab>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    588\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_pids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_cids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_labels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m                       \u001b[0mpids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_pids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m                       \u001b[0mG_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mGNN_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAll_ProteinData_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;31m#pass each unique complex through the GNN once\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m                       \u001b[0mGNN_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mG_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#append to make examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m                       \u001b[0;32mdel\u001b[0m \u001b[0mG_dict\u001b[0m \u001b[0;31m#clear up memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-aa9bb44951ab>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m         \u001b[0mx1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m         \u001b[0mx2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0mx3\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-aa9bb44951ab>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0msame_norm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msame_norm\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0mdiff_norm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdiff_norm\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m         \u001b[0mneigh_same_atoms_signal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msame_neigh_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0msame_norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m         \u001b[0mneigh_diff_atoms_signal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiff_neigh_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdiff_norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Tue Jan  9 13:06:21 2024\n",
        "\n",
        "@author: u1876024\n",
        "\"\"\"\n",
        "\n",
        "import warnings\n",
        "from Bio.PDB import *\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import glob\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import pdb\n",
        "import warnings\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import glob\n",
        "from Bio.PDB import *\n",
        "import warnings\n",
        "from Bio.PDB import *\n",
        "import numpy as np\n",
        "from Bio.PDB.NeighborSearch import NeighborSearch\n",
        "from tqdm import tqdm as tqdm\n",
        "import pickle\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import normalize\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.metrics import auc,precision_recall_curve\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.metrics import roc_auc_score,roc_curve,precision_score,recall_score,average_precision_score\n",
        "import pickle\n",
        "from rdkit import Chem\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "from torch.autograd import Variable\n",
        "def cuda(v):\n",
        "    if USE_CUDA:\n",
        "        return v.cuda()\n",
        "    return v\n",
        "def toTensor(v,dtype = torch.float,requires_grad = False):\n",
        "    return cuda(Variable(torch.tensor(v)).type(dtype).requires_grad_(requires_grad))\n",
        "def toNumpy(v):\n",
        "    if USE_CUDA:\n",
        "        return v.detach().cpu().numpy()\n",
        "    return v.detach().numpy()\n",
        "\n",
        "'''\n",
        "Using Sklearn One hot encoder to encode the atoms\n",
        "Output is of size N*M where N is the total number of atoms and M is the total number of encoded features\n",
        "'''\n",
        "def atom1(structure):\n",
        "    atomslist=np.array(sorted(np.array(['C', 'CA', 'CB', 'CG', 'CH2', 'N','NH2',  'OG','OH', 'O1', 'O2', 'SE','1']))).reshape(-1,1)\n",
        "    enc = OneHotEncoder(handle_unknown='ignore')\n",
        "    enc.fit(atomslist)\n",
        "    atom_list=[]\n",
        "    for atom in structure.get_atoms():\n",
        "        if atom.get_name() in atomslist:\n",
        "            atom_list.append(atom.get_name())\n",
        "        else:\n",
        "            atom_list.append(\"1\")\n",
        "    atoms_onehot=enc.transform(np.array(atom_list).reshape(-1,1)).toarray()\n",
        "    return atoms_onehot\n",
        "##############\n",
        "'''\n",
        "One hot encoded residue infomration using SKlearn Library\n",
        "\n",
        "Output is N*M where N is the total number of atoms and M is the encoded features of the residues.\n",
        "Any unknown  residue is mapped to 1\n",
        "'''\n",
        "\n",
        "\n",
        "def res1(structure):\n",
        "    residuelist=np.array(sorted(np.array(['ALA', 'ARG', 'ASN', 'ASP', 'GLN', 'GLU', 'GLY', 'ILE', 'LEU', 'LYS', 'MET', 'PHE', 'PRO', 'SER', 'THR', 'TRP', 'TYR', 'VAL', 'CYS', 'HIS','1']))).reshape(-1,1)\n",
        "    encr = OneHotEncoder(handle_unknown='ignore')\n",
        "    encr.fit(residuelist)\n",
        "    residue_list=[]\n",
        "    for atom in structure.get_atoms():\n",
        "        if atom.get_parent().get_resname() in residuelist:\n",
        "            residue_list.append((atom.get_parent()).get_resname())\n",
        "        else:\n",
        "            residue_list.append(\"1\")\n",
        "\n",
        "    res_onehot=encr.transform(np.array(residue_list).reshape(-1,1)).toarray()\n",
        "\n",
        "    return res_onehot\n",
        "###########\n",
        "\n",
        "'''\n",
        "It calculates the neighbours of each atom i.e. 10 distinct neighbours\n",
        "Output is  in the form of a ditionary representing an  adjacency list where each source atom and neighbouring atom is represented bby its sequence index .\n",
        "'''\n",
        "\n",
        "\n",
        "def neigh1(structure):\n",
        "    #atom_list is a numpy array  that   contains all the atoms of the pdb file in atom object\n",
        "    atom_list=np.array([atom for atom in structure.get_atoms()])\n",
        "\n",
        "    #for atom in structure.get_atoms():\n",
        "    #    atom_list.append(atom)\n",
        "    #neighbour_list contains all the  neighbour atomic pairs  i.e. like if N has neighbours O and C then it is stored as [[N,C],[N,O]] i.e. has dimension N*2 where N is the total number of possible neighbours all the atoms have in an unsorted manner and it stores in the form of  atom object\n",
        "\n",
        "\n",
        "    p4=NeighborSearch(atom_list)\n",
        "    neighbour_list=p4.search_all(6,level=\"A\")\n",
        "    neighbour_list=np.array(neighbour_list)\n",
        "\n",
        "    #dist is the distance between the neighbour and the source atom  i.e. dimension is N*1\n",
        "    dist=np.array(neighbour_list[:,0]-neighbour_list[:,1])\n",
        "    #sorting in ascending order\n",
        "    place=np.argsort(dist)\n",
        "    sorted_neighbour_list=neighbour_list[place]\n",
        "\n",
        "    #old_atom_number is used for  storing atom id of the original protein before sorting\n",
        "    #old_residue_number is used for storing residue number of the original protein before sorting\n",
        "    source_vertex_list_atom_object=np.array(sorted_neighbour_list[:,0])\n",
        "    len_source_vertex=len(source_vertex_list_atom_object)\n",
        "    neighbour_vertex_with_respect_each_source_atom_object=np.array(sorted_neighbour_list[:,1])\n",
        "    old_atom_number=[]\n",
        "    old_residue_number=[]\n",
        "    for i in atom_list:\n",
        "        old_atom_number.append(i.get_serial_number())\n",
        "        old_residue_number.append(i.get_parent().get_id()[1])\n",
        "    old_atom_number=np.array(old_atom_number)\n",
        "    old_residue_number=np.array(old_residue_number)\n",
        "    req_no=len(neighbour_list)\n",
        "    total_atoms=len(atom_list)\n",
        "    #neigh_same_res is the 2D numpy array to store the indices of the  neighbours of  same residue and is of the shape N*10 where N is the total number of atoms\n",
        "    #neigh_diff_res is 2D numpy array to store  the indices of the  neighbours of different residue\n",
        "    #same_flag is used to restrict the neighbours belonging to same residue  to 10\n",
        "    #diff_flag is used to restrict the neighbours belonging to different residue to 10\n",
        "    neigh_same_res=np.array([[-1]*10 for i in range(total_atoms)])\n",
        "    neigh_diff_res=np.array([[-1]*10 for i in range(total_atoms)])\n",
        "    same_flag=[0]*total_atoms\n",
        "    diff_flag=[0]*total_atoms\n",
        "    for i in range(len_source_vertex):\n",
        "        source_atom_id=source_vertex_list_atom_object[i].get_serial_number()\n",
        "        neigh_atom_id=neighbour_vertex_with_respect_each_source_atom_object[i].get_serial_number()\n",
        "        source_atom_res=source_vertex_list_atom_object[i].get_parent().get_id()[1]\n",
        "        neigh_atom_res=neighbour_vertex_with_respect_each_source_atom_object[i].get_parent().get_id()[1]\n",
        "        #finding out index of the source and neighbouring atoms from the original atom array with respect to their residue id and atom id\n",
        "        temp_index1=np.where(source_atom_id==old_atom_number)[0]\n",
        "\n",
        "        temp_index2=np.where(neigh_atom_id==old_atom_number)[0]\n",
        "        for i1 in temp_index1:\n",
        "            if old_residue_number[i1]==source_atom_res:\n",
        "                source_index=i1\n",
        "                break\n",
        "        for i1 in temp_index2:\n",
        "            if old_residue_number[i1]==neigh_atom_res:\n",
        "                neigh_index=i1\n",
        "                break\n",
        "        #if both the residues are same\n",
        "\n",
        "        if source_atom_res==neigh_atom_res :\n",
        "\n",
        "            #limiting the number of neighbours of same residue to 10\n",
        "\n",
        "            if int(same_flag[source_index])< 10:\n",
        "                neigh_same_res[source_index][same_flag[source_index]]=neigh_index\n",
        "                same_flag[source_index]+=1\n",
        "\n",
        "            if int(same_flag[neigh_index])< 10:\n",
        "                neigh_same_res[neigh_index][same_flag[neigh_index]]=source_index\n",
        "                same_flag[neigh_index]+=1\n",
        "\n",
        "        # if both the residues are different\n",
        "        elif source_atom_res!=neigh_atom_res :\n",
        "\n",
        "            #limiting the number of neighbours of different residues to 10\n",
        "\n",
        "            if int(diff_flag[source_index])< 10:\n",
        "                neigh_diff_res[source_index][diff_flag[source_index]]=neigh_index\n",
        "                diff_flag[source_index]+=1\n",
        "\n",
        "\n",
        "            if int(diff_flag[neigh_index])< 10:\n",
        "\n",
        "                neigh_diff_res[neigh_index][diff_flag[neigh_index]]=source_index\n",
        "                diff_flag[neigh_index]+=1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return neigh_same_res,neigh_diff_res\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA is available. Number of GPUs: {torch.cuda.device_count()}\")\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
        "else:\n",
        "    print(\"CUDA is not available.\")\n",
        "class GNN_Layer(nn.Module):\n",
        "\n",
        "    def __init__(self, filters, v_feats, trainable=True, **kwargs):\n",
        "        #pdb.set_trace()\n",
        "        super(GNN_Layer, self).__init__()\n",
        "        self.v_feats = v_feats\n",
        "        self.filters = filters\n",
        "\n",
        "        self.trainable = trainable\n",
        "        use_cuda = torch.cuda.is_available()\n",
        "        device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "        self.cuda_device=device\n",
        "        self.Wsv = nn.Parameter( torch.randn(self.v_feats, self.filters, device=self.cuda_device,requires_grad=True))\n",
        "        self.Wdr = nn.Parameter( torch.randn(self.v_feats, self.filters, device=self.cuda_device,requires_grad=True))\n",
        "        self.Wsr = nn.Parameter( torch.randn(self.v_feats, self.filters, device=self.cuda_device,requires_grad=True))\n",
        "        self.neighbours=10\n",
        "        #print(\"Wsv shape\",self.Wsv.shape)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #pdb.set_trace()\n",
        "        Z,same_neigh,diff_neigh = x\n",
        "        #print(\"input\",x)\n",
        "        #print(Z.shape)\n",
        "        node_signals = Z@self.Wsv\n",
        "        neigh_signals_same=Z@self.Wsr\n",
        "        neigh_signals_diff=Z@self.Wdr\n",
        "        unsqueezed_same_neigh_indicator=(same_neigh>-1).unsqueeze(2)\n",
        "        unsqueezed_diff_neigh_indicator=(diff_neigh>-1).unsqueeze(2)\n",
        "        same_neigh_features=neigh_signals_same[same_neigh]*unsqueezed_same_neigh_indicator\n",
        "        diff_neigh_features=neigh_signals_diff[diff_neigh]*unsqueezed_diff_neigh_indicator\n",
        "        same_norm = torch.sum(same_neigh > -1, 1).unsqueeze(1).type(torch.float)\n",
        "        diff_norm = torch.sum(diff_neigh > -1, 1).unsqueeze(1).type(torch.float)\n",
        "\n",
        "        # To prevent divide by zero error\n",
        "        same_norm[same_norm==0]=1\n",
        "        diff_norm[diff_norm==0]=1\n",
        "        neigh_same_atoms_signal = (torch.sum(same_neigh_features, axis=1))/same_norm\n",
        "        neigh_diff_atoms_signal = (torch.sum(diff_neigh_features, axis=1))/diff_norm\n",
        "        final_res = torch.relu(node_signals +neigh_same_atoms_signal+neigh_diff_atoms_signal)\n",
        "\n",
        "        return final_res,same_neigh,diff_neigh\n",
        "\n",
        "class GNN_First_Layer(nn.Module):\n",
        "\n",
        "    def __init__(self, filters, trainable=True, **kwargs):\n",
        "\n",
        "        super(GNN_First_Layer, self).__init__()\n",
        "        self.filters = filters\n",
        "        #pdb.set_trace()\n",
        "        self.trainable = trainable\n",
        "        use_cuda = torch.cuda.is_available()\n",
        "        device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "        self.cuda_device = device\n",
        "        self.Wv = nn.Parameter( torch.randn(13, self.filters, device=self.cuda_device,requires_grad=True))\n",
        "        self.Wr = nn.Parameter( torch.randn(21,self.filters, device=self.cuda_device,requires_grad=True))\n",
        "        self.Wsr= nn.Parameter( torch.randn(13, self.filters, device=self.cuda_device,requires_grad=True))\n",
        "        self.Wdr= nn.Parameter( torch.randn(13, self.filters, device=self.cuda_device,requires_grad=True))\n",
        "        self.neighbours=10\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        #pdb.set_trace()\n",
        "        #print(\"Here i am in froward of first layer\")\n",
        "        atoms, residues,same_neigh,diff_neigh = x\n",
        "        #print(\"input atoms\",atoms)\n",
        "        #print(atoms.shape)\n",
        "        #print(\"Wv shape\",self.Wv.shape)\n",
        "        node_signals = atoms@self.Wv\n",
        "        ####\n",
        "        #print(\"input residues\",residues)\n",
        "        #print(residues.shape)\n",
        "        #print(\"Wr shape\",self.Wr.shape)\n",
        "        ####\n",
        "        residue_signals = residues@self.Wr\n",
        "        #print(\"Wsr shape\",self.Wsr.shape)\n",
        "        #print(\"Wdr shape\",self.Wdr.shape)\n",
        "        neigh_signals_same=atoms@self.Wsr\n",
        "        neigh_signals_diff=atoms@self.Wdr\n",
        "        #print(\"neigh_signals_same shape\",neigh_signals_same.shape)\n",
        "        ####\n",
        "        unsqueezed_same_neigh_indicator=(same_neigh>-1).unsqueeze(2)\n",
        "        unsqueezed_diff_neigh_indicator=(diff_neigh>-1).unsqueeze(2)\n",
        "        \"\"\"\n",
        "        unsqueezed_same_neigh_indicator=(same_neigh>-1).unsqueeze(1)\n",
        "        unsqueezed_diff_neigh_indicator=(diff_neigh>-1).unsqueeze(1)\n",
        "        \"\"\"\n",
        "        same_neigh_features=neigh_signals_same[same_neigh]*unsqueezed_same_neigh_indicator\n",
        "        diff_neigh_features=neigh_signals_diff[diff_neigh]*unsqueezed_diff_neigh_indicator\n",
        "        #print(\"same norm\",same_neigh > -1, 1)\n",
        "        same_norm = torch.sum(same_neigh > -1, 1).unsqueeze(1).type(torch.float)\n",
        "        diff_norm = torch.sum(diff_neigh > -1, 1).unsqueeze(1).type(torch.float)\n",
        "        same_norm = torch.sum(same_neigh > -1).type(torch.float)\n",
        "        diff_norm = torch.sum(diff_neigh > -1).type(torch.float)\n",
        "        # To prevent divide by zero error\n",
        "        same_norm[same_norm==0]=1\n",
        "        diff_norm[diff_norm==0]=1\n",
        "        neigh_same_atoms_signal=(torch.sum(same_neigh_features, axis=1))/same_norm\n",
        "        neigh_diff_atoms_signal=(torch.sum(diff_neigh_features, axis=1))/diff_norm\n",
        "\n",
        "        final_res = torch.relu(node_signals+residue_signals +neigh_same_atoms_signal+neigh_diff_atoms_signal)\n",
        "\n",
        "        return final_res, same_neigh,diff_neigh\n",
        "\n",
        "\n",
        "class Dense(nn.Module):\n",
        "    def __init__(self, in_dims, out_dims, trainable=True, **kwargs):\n",
        "        #pdb.set_trace()\n",
        "        super(Dense, self).__init__()\n",
        "        self.in_dims = in_dims\n",
        "        self.out_dims = out_dims\n",
        "        use_cuda = torch.cuda.is_available()\n",
        "        device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "        self.cuda_device = device\n",
        "\n",
        "        self.W = nn.Parameter( torch.randn(self.in_dims, self.out_dims, device=self.cuda_device,requires_grad=True))\n",
        "\n",
        "    def forward(self, x):\n",
        "        #pdb.set_trace()\n",
        "        Z = torch.sigmoid(torch.matmul(x, self.W))\n",
        "\n",
        "        return Z\n",
        "\n",
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GNN, self).__init__()\n",
        "        self.conv1 = GNN_First_Layer(filters=512)\n",
        "        self.conv2 = GNN_Layer(v_feats=512, filters=1024)\n",
        "        self.conv3 = GNN_Layer(v_feats=1024, filters=512)\n",
        "        self.dense = Dense(in_dims=512, out_dims=1)\n",
        "    def forward(self, x):\n",
        "        x1=self.conv1(x)\n",
        "        x2=self.conv2(x1)\n",
        "        x3=self.conv3(x2)\n",
        "        x=x3[0]\n",
        "        x=torch.sum(x,axis=0).view(1,-1)\n",
        "        x = F.normalize(x)\n",
        "        return x\n",
        "\n",
        "    def processProtein(UniqueProtein, PdBloc):#, PdBloc):\n",
        "        data_list = []\n",
        "\n",
        "        use_cuda = torch.cuda.is_available()\n",
        "        device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "        PData_dict={}\n",
        "        for i in range(len(UniqueProtein)):\n",
        "            #print('Converting PDB to Graph: {}/{}'.format(i+1, len(UniqueProtein)))\n",
        "            UniqueProtein[i]=UniqueProtein[i].split('.pdb')[0]\n",
        "            P1=PdBloc+UniqueProtein[i]+'.pdb'\n",
        "            #if P1 in UniqueProtein:\n",
        "            parser = PDBParser()\n",
        "            with warnings.catch_warnings(record=True) as w:\n",
        "              structure = parser.get_structure(\"\", P1)\n",
        "            one_hot_atom=(atom1(structure))\n",
        "\n",
        "            one_hot_res=(res1(structure))\n",
        "            #print(\"\\none_hot_atom.shape\",one_hot_res.shape)\n",
        "            neigh_same_res,neigh_diff_res=(neigh1(structure))\n",
        "            #print(\"\\neigh_same_res.shape,neigh_diff_re.shape\",neigh_same_res.shape,\"\\n\",neigh_diff_res.shape)\n",
        "            # make the graph ready for PyTorch Geometrics GCN algorithms:\n",
        "            one_hot_atom=torch.tensor(one_hot_atom,dtype=torch.float32).to(device)\n",
        "            one_hot_res=torch.tensor(one_hot_res,dtype=torch.float32).to(device)\n",
        "            neigh_same_res=torch.tensor(neigh_same_res).to(device).long()\n",
        "            neigh_diff_res=torch.tensor(neigh_diff_res).to(device).long()\n",
        "            GNNData = [one_hot_atom,one_hot_res,neigh_same_res,neigh_diff_res]\n",
        "            #import pdb; pdb.set_trace()\n",
        "            #GNNData.__setitem__('Total atoms', len(one_hot_atom))\n",
        "            #data_list.append(GNNData)\n",
        "            PData_dict[UniqueProtein[i]]= GNNData\n",
        "        return PData_dict\n",
        "def readFile(filename):\n",
        "  with open(filename) as f:\n",
        "    D = f.readlines()\n",
        "  Name=[];PdbId=[];Ligandnames=[];SMILES=[];labels=[];\n",
        "  All_data_list=[]\n",
        "  from tqdm import tqdm as tqdm\n",
        "  #2XA0_A_2_B 2O21 2XA0 43B c1ccc(cc1)CCc2nc3cc(ccc3s2)c4ccc(cc4)C(=O)NS(=O)(=O)c5ccc(c(c5)[N](=O)[O-])NCCSc6ccccc6  1\n",
        "  for d in tqdm(D):\n",
        "      #if len(d)==6:\n",
        "      name,inhibtedc,Pdbid,Ligandid,smiles,y = d.split()\n",
        "      Name.append(name);PdbId.append(Pdbid);Ligandnames.append(Ligandid);SMILES.append(smiles);labels.append(float (y));\n",
        "  return  PdbId,Ligandnames,SMILES,labels\n",
        "class IPPI_MLP_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(IPPI_MLP_Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(2840, 1024)#4096)\n",
        "        self.fc2 = nn.Linear(1024, 512)\n",
        "        self.fc3 = nn.Linear(512, 100)\n",
        "        self.fc6 = nn.Linear(100, 1)\n",
        "    def forward(self, PFeatures,LigandFeatures,ProteinInterfaceF):\n",
        "          Cfeatures=LigandFeatures\n",
        "          P_all_Features=torch.hstack((PFeatures,ProteinInterfaceF))\n",
        "          PC_Features=torch.hstack((P_all_Features,Cfeatures))\n",
        "          x = torch.tanh(self.fc1(PC_Features))\n",
        "          x = torch.tanh(self.fc2(x))\n",
        "          x = torch.relu(self.fc3(x))\n",
        "          x = self.fc6(x)\n",
        "          return x\n",
        "\n",
        "#path,githubpath='/content/drive/MyDrive/GNN-PPI-Inhibitor/','/content/PPI-Inhibitors/'\n",
        "path,githubpath='./','./PPI-Inhibitors/'\n",
        "\"\"\"\n",
        "path = location of pkl files from these links:\n",
        "https://drive.google.com/file/d/1goeDiPZSKT1Xx3j00eNG9xlqYkLLv1gW/view\n",
        "https://drive.google.com/file/d/1GOYEKLQCoGea9QQ72kujy0rdJKbUSYAE/view\n",
        "\n",
        "githubpath =  location of the directory containing the github repo PPI-Inhibitors\n",
        "obtained using\n",
        "git clone https://github.com/adibayaseen/PPI-Inhibitors\n",
        "\"\"\"\n",
        "Ubench5InterfaceandSeq_dict=pickle.load(open(githubpath+'Features/NewUbench5InterfaceandSeq_dict.npy',\"rb\"))\n",
        "Pos_seqandInterfaceF_dict=pickle.load(open(githubpath+'Features/Pos_seqandInterfaceF_dict.npy',\"rb\"))\n",
        "Complex_AllFeatures_dict=dict( list (Pos_seqandInterfaceF_dict.items())+list (Ubench5InterfaceandSeq_dict.items()))\n",
        "##############\n",
        "ComplexInterfaceFeatures={}\n",
        "for key in Complex_AllFeatures_dict:\n",
        "  if len(key.split('_'))>1:\n",
        "    compname=key.split('_')[0]\n",
        "    ComplexInterfaceFeatures[compname]=Complex_AllFeatures_dict[key]#.cuda()\n",
        "  else:\n",
        "    ComplexInterfaceFeatures[key]=Complex_AllFeatures_dict[key]#.cuda()\n",
        "CompoundFingerprintFeaturesDict=pickle.load(open(githubpath+'Features/Compound_Fingerprint_Features_Dict.npy',\"rb\"))\n",
        "#Load Protein data for GNN\n",
        "path='/content/drive/MyDrive/GNN-PPI-Inhibitor/'\n",
        "ProteinDataGNN_dict=pickle.load(open(path+'ProteinData_dict.pickle',\"rb\"))\n",
        "DBD5_ProteinDataGNN_dict=pickle.load(open(path+'DBD5_ProteinData_dict.pickle',\"rb\"))\n",
        "All_ProteinData_dict=dict( list (ProteinDataGNN_dict.items())+list (DBD5_ProteinDataGNN_dict.items()))\n",
        "for d in All_ProteinData_dict:\n",
        "  data=All_ProteinData_dict[d]\n",
        "  All_ProteinData_dict[d]=[data[0].cuda(),data[1].cuda(),data[2].cuda(),data[3].cuda()]\n",
        "#########\n",
        "with open(githubpath+'Data/WriteAllexamplesRandomBindersIdsAll_24JAN_Binary.txt') as f:\n",
        "#with open(githubpath+'Data/WriteAllexamplesRandomBindersIdsAll_24JAN.txt') as f:\n",
        "    D = f.readlines()\n",
        "Labels=[];Ligandnames=[];Complexs=[];TestPoscomplexes=[];#SMILESlist=[];\n",
        "for d in tqdm(D):\n",
        "  if len(d.split())==4:\n",
        "      TestPoscomp,Complexname,Ligandname,label = d.split()\n",
        "  else:\n",
        "      TestPoscomp,Complexname,Ligandname,label = d.split()[0],d.split()[1],(' ').join(d.split()[2:-1]),d.split()[-1]\n",
        "  TestPoscomplexes.append(TestPoscomp),Ligandnames.append(Ligandname);Complexs.append(Complexname);Labels.append(float (label))\n",
        "#########Make dictionary, Rootcomplexname=(complexname,compoundname),label\n",
        "Allexamples=dict (zip(zip(TestPoscomplexes,zip(Complexs,Ligandnames)),Labels))\n",
        "#Group kfold\n",
        "Alldata=list (Allexamples.keys())\n",
        "KK=[k[0].split('_')[0] for k in Alldata]\n",
        "groups = pd.DataFrame(KK)\n",
        "gkf = GroupKFold(n_splits=len(set (KK)))\n",
        "###########\n",
        "AUC_ROC_final=[];Avg_P_final=[];Z=[];Yo=[];Y_t=[];Y_score=[];\n",
        "from os import listdir\n",
        "AlltestExamples=[];Externallabels=[];ExternalscoresLOCO=[];covid19_Externallabels=[];covid19_ExternalscoresLOCO=[];Y_score=[];Y_t=[];classratio_dict={};\n",
        "AUC_ROC_final=[];Avg_P_final=[];\n",
        "Complexs,Ligandnames, Labels=np.array(Complexs),np.array(Ligandnames),np.array(Labels)\n",
        "Alldata=np.array(Alldata)\n",
        "classratio_dict=pickle.load(open(githubpath+'Features/Classratio_GNNdict.npy','rb'))\n",
        "\n",
        "#%% Cross-validation\n",
        "Done=set(KK).difference(['3D9T','1BKD','4ESG','2FLU','1YCQ','2XA0','3TDU','3D9T','2B4J','3DAB','3UVW','2RNY','4AJY', '1F47','1YCR','4QC3','1NW9','2E3K','4YY6','4GQ6','3WN7','1BXL','1Z92'])\n",
        "for trainindex, testindex in gkf.split(KK, KK, groups=groups):\n",
        "    train,test=Alldata[trainindex],Alldata[testindex]\n",
        "\n",
        "    if test[0][0].split('_')[0] in Done:\n",
        "      continue\n",
        "\n",
        "    Ctr=[];Ptr=[];y_train=[];Ctrname=[];Ptrname=[];Xtr=[];G=[];Cttname=[];Ctt=[];y_test=[];Ptt=[];Pttname=[];\n",
        "    #Split train and test\n",
        "    for t in train:\n",
        "        Ctrname.append(t[1][1]);Ctr.append(CompoundFingerprintFeaturesDict[t[1][1]]);\n",
        "        #change this only for GNN Complex_AllFeatures_dict with All_ProteinData_dic and t\n",
        "        #####\n",
        "        GNNcomp=t[1][0].split('_')[0]#t[1][0].split('_')[0]\n",
        "        Ptrname.append(GNNcomp);Ptr.append(ComplexInterfaceFeatures[GNNcomp]);\n",
        "        y_train.append(Allexamples[t[0],t[1]])\n",
        "    #Split train and test\n",
        "    for t in test:\n",
        "        GNNcomp=t[1][0].split('_')[0]\n",
        "        Cttname.append(t[1][1]);Ctt.append(CompoundFingerprintFeaturesDict[t[1][1]]);\n",
        "        Pttname.append(GNNcomp);Ptt.append(ComplexInterfaceFeatures[GNNcomp]);\n",
        "        y_test.append(Allexamples[t[0],t[1]])\n",
        "    #standarization\n",
        "    Pscaler = StandardScaler().fit(Ptr)\n",
        "    Cscaler = StandardScaler().fit(Ctr)\n",
        "    Ctr = Cscaler.transform(Ctr)\n",
        "    Ptr=Pscaler.transform(Ptr)\n",
        "    Ptt=Pscaler.transform(Ptt)\n",
        "    Ptrdict=dict (zip(Ptrname,torch.FloatTensor(Ptr).cuda()))\n",
        "    Ctrdict=dict (zip (Ctrname,torch.FloatTensor( Ctr).cuda()))\n",
        "    Ctt = Cscaler.transform(Ctt)\n",
        "    Cttdict=dict (zip (Cttname,torch.FloatTensor( Ctt).cuda()))\n",
        "    Pttdict=dict (zip(Pttname,torch.FloatTensor(Ptt).cuda()))\n",
        "\n",
        "    y_train = np.array(y_train)\n",
        "\n",
        "    IPPI_Net = IPPI_MLP_Net().cuda()\n",
        "\n",
        "    GNN_model=GNN().cuda()\n",
        "    Mcomplexname=test[0][0].split('_')[0]\n",
        "    criterion  = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.Adam(list (IPPI_Net.parameters()) + list( GNN_model.parameters()),lr=0.001,weight_decay=0.0)#001)#0.69 for 1mer single layer#, weight_decay=0.01, betas=(0.9, 0.999))\n",
        "\n",
        "    bsize = 1024\n",
        "\n",
        "\n",
        "\n",
        "    dataset = CustomDataset(train[:,1], y_train.astype('int'))\n",
        "    batch_sampler = BinaryBalancedSampler(y_train.astype('int'), bsize)\n",
        "    loader = DataLoader(dataset, batch_sampler=batch_sampler) # data loader that selects equal number of positive and negative examples\n",
        "\n",
        "    test_dataset = CustomDataset(test[:,1], np.array(y_test).astype('int'))\n",
        "    test_loader = DataLoader(test_dataset, batch_size=bsize, shuffle=False)\n",
        "\n",
        "\n",
        "    #y_train=torch.FloatTensor( y_train).cuda()\n",
        "    ####\n",
        "    print (\"test complex \", Mcomplexname)\n",
        "\n",
        "    Loss = [] #save loss values for plotting\n",
        "    E = [] #save examples\n",
        "    L = [] #save labels\n",
        "    terminated = False\n",
        "    best_result = 0.0\n",
        "    best_model = None\n",
        "    counter = 0\n",
        "    early_stop_count = 0\n",
        "    Zlist,Ylist=[],[]\n",
        "    for iters in tqdm(range(5)):\n",
        "        for (batch_pids,batch_cids),batch_labels in tqdm(loader):\n",
        "            GNN_model.train()\n",
        "            IPPI_Net.train()\n",
        "            E.extend(zip(batch_pids,batch_cids))\n",
        "            L.append(batch_labels)\n",
        "            pids = [p.split('_')[0] for p in batch_pids]\n",
        "            G_dict = {p:GNN_model(All_ProteinData_dict[p]) for p in set(pids)} #pass each unique complex through the GNN once\n",
        "            GNN_features = torch.vstack([G_dict[p] for p in pids]) #append to make examples\n",
        "            del G_dict #clear up memory\n",
        "            interface_features = torch.vstack([Ptrdict[p] for p in pids])\n",
        "            compound_features = torch.vstack([Ctrdict[c] for c in batch_cids])\n",
        "            #[GNN_model(All_ProteinData_dict[p]) for p in set_pids]\n",
        "            output = IPPI_Net(GNN_features,compound_features,interface_features)\n",
        "            V = np.min(list(classratio_dict.values()))\n",
        "            weights = toTensor(np.array([classratio_dict[p]/V if batch_labels[i]==1 else 1.0 for i,p in enumerate(pids)  ]))\n",
        "            criterion  = nn.BCEWithLogitsLoss(weight = None)\n",
        "            loss = criterion(output.flatten(), batch_labels.float().cuda())\n",
        "            Loss.append(loss.item())\n",
        "            #if np.median(Loss[-10:])<1e-1: terminated = True\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            early_stop_count += 1\n",
        "            #%% Validation/Testing (saves the best model in every 10 iterations over the validation set)\n",
        "            GNN_model.eval()\n",
        "            IPPI_Net.eval()\n",
        "            Z, Y = [], []\n",
        "            with torch.no_grad():\n",
        "                for (batch_pids,batch_cids),batch_labels in test_loader:\n",
        "                    pids = [p.split('_')[0] for p in batch_pids]\n",
        "                    G_dict = {p:GNN_model(All_ProteinData_dict[p]) for p in set(pids)} #pass each unique complex through the GNN once\n",
        "                    GNN_features = torch.vstack([G_dict[p] for p in pids]) #append to make examples\n",
        "                    del G_dict #clear up memory\n",
        "                    interface_features = torch.vstack([Pttdict[p] for p in pids])\n",
        "                    compound_features = torch.vstack([Cttdict[c] for c in batch_cids])\n",
        "                    output = IPPI_Net(GNN_features,compound_features,interface_features)\n",
        "                    Z.extend(output.cpu().flatten().numpy())\n",
        "                    Y.extend(batch_labels.cpu().flatten().numpy())\n",
        "                aucroc = roc_auc_score(np.array(Y), np.array(Z))\n",
        "                aucpr = average_precision_score(Y,Z)\n",
        "                if aucroc>best_result:\n",
        "                    early_stop_count = 0\n",
        "                    best_result = aucroc\n",
        "                    best_model = (GNN_model.state_dict(),IPPI_Net.state_dict())\n",
        "                    IPPI_Net.load_state_dict(best_model[1])#path+'/newIPPI_Net_'+test[0][0].split('_')[0]+'_AUC_'+str (round (best_result,3)))#torch.load(path+'IPPI_Net_'+ Mcomplexname)[1])\n",
        "                    GNN_model.load_state_dict(best_model[0])\n",
        "                    GNN_model.eval()\n",
        "                    IPPI_Net.eval()\n",
        "                    Zb, Yb = [], []\n",
        "                    for (batch_pids,batch_cids),batch_labels in test_loader:\n",
        "                      pids = [p.split('_')[0] for p in batch_pids]\n",
        "                      G_dict = {p:GNN_model(All_ProteinData_dict[p]) for p in set(pids)} #pass each unique complex through the GNN once\n",
        "                      GNN_features = torch.vstack([G_dict[p] for p in pids]) #append to make examples\n",
        "                      del G_dict #clear up memory\n",
        "                      interface_features = torch.vstack([Pttdict[p] for p in pids])\n",
        "                      compound_features = torch.vstack([Cttdict[c] for c in batch_cids])\n",
        "                      bestmodeloutput=IPPI_Net(GNN_features,compound_features,interface_features)\n",
        "                      #torch.save(Loss, path+'/Loss_'+test[0][0].split('_')[0])\n",
        "                      Zb.extend(bestmodeloutput.cpu().flatten().numpy())\n",
        "                      Yb.extend(batch_labels.cpu().flatten().numpy())\n",
        "                    aucrocb = roc_auc_score(np.array(Yb), np.array(Zb))\n",
        "                    aucprb = average_precision_score(Yb,Zb)\n",
        "                    print('LOADED BEST AUCROC',aucrocb,'AUCPR',aucprb)#,'best aucroc')\n",
        "                    aucpr = average_precision_score(Y,Z)\n",
        "                    print('AUCROC',aucroc,'AUCPR',aucpr,'best aucroc',best_result)\n",
        "    ###Load best model\n",
        "    print (\"OUTSIDE LOOP AUC of Best\")\n",
        "    Zlist.extend(Zb);Ylist.extend(Yb)\n",
        "    np.save(path+test[0][0].split('_')[0]+'Scores',Zb)\n",
        "    np.save(path+test[0][0].split('_')[0]+'Targets',Yb)\n",
        "    aucrocb = roc_auc_score(np.array(Yb), np.array(Zb))\n",
        "    aucprb = average_precision_score(Yb,Zb)\n",
        "    print('Complex name',test[0][0].split('_')[0],'AUCROC',aucrocb,'AUCPR',aucprb)#,'best aucroc')\n",
        "fpr, tpr, thresholds = roc_curve(Ylist, Zlist)\n",
        "Auc = roc_auc_score(Ylist, Zlist)\n",
        "Auc=(Auc).round(2)\n",
        "# calculate precision-recall curve\n",
        "Zlist=np.array(Zlist);Yo=np.array(Ylist);\n",
        "#Y_t=np.array(Y_t);Y_score=np.array(Y_score)\n",
        "precision, recall, thresholds = precision_recall_curve(Ylist, Zlist)\n",
        "aucpr=average_precision_score (Ylist, Zlist)\n",
        "########\n",
        "np.save(path+'GNN-pipeline_Targets.npy',Ylist)\n",
        "np.save(path+'GNN-pipeline_Scores.npy',Zlist)\n",
        "######+\n",
        "fig = plt.figure()\n",
        "plt.plot(recall,precision,color='m',marker=',',label='AUC-PR:{: .2f}'.format(aucpr))\n",
        "plt.title('AUC-PR');plt.xlabel('recall');plt.ylabel('precision');plt.grid();plt.legend();plt.show();\n",
        "fig .savefig(path+\"GNN-pipeline AUC-PR for PPI Inhibitors.pdf\", bbox_inches='tight')\n",
        "###\n",
        "aucpr=(aucpr).round(2)\n",
        "print(\"AucROC and aucpr Over all complexes \\n\",Auc,\"\\n\",aucpr,\"\\ntotal P:N ration 1:\",int (np.sum([Yo==-1.0])/np.sum([Yo==1.0])))#,\"\\n\")\n",
        "#######\n",
        "fig = plt.figure()\n",
        "plt.plot(fpr,tpr,color='k',marker='d',label='AUC:{: .2f}'.format(Auc))\n",
        "plt.title('AUCROC');plt.xlabel('FPR');plt.ylabel('TPR');plt.grid();plt.legend();plt.show();\n",
        "fig .savefig(path+\"GNN-pipeline AUCROC for vPPI Inhibitors.pdf\", bbox_inches='tight')\n",
        "###\n",
        "print(\"Final average over all folds,Leave one complex out\",p.average(AUC_ROC_final).round(4),'±',np.std( AUC_ROC_final).round(4),np.average(Avg_P_final).round(4),'±',np.std( Avg_P_final).round(4))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score,roc_curve,precision_score,recall_score,average_precision_score,precision_recall_curve,auc\n",
        "import matplotlib.pyplot as plt\n",
        "path,githubpath='/content/drive/MyDrive/GNN-PPI-Inhibitor/','/content/PPI-Inhibitors/'\n",
        "Z_GearNet=np.load(path+'Z_GearNet.npy')\n",
        "Yo_GearNet=np.load(path+'Yo_GearNet.npy')\n",
        "####\n",
        "fpr_GearNet, tpr_GearNet, thresholds_GearNet = roc_curve(Yo_GearNet, Z_GearNet)\n",
        "Auc_GearNet = roc_auc_score(Yo_GearNet, Z_GearNet)\n",
        "Auc_GearNet=(Auc_GearNet).round(2)\n",
        "# calculate precision-recall curve\n",
        "precision_GearNet, recall_GearNet, thresholds = precision_recall_curve(Yo_GearNet, Z_GearNet)\n",
        "aucpr_GearNet=auc(recall_GearNet,precision_GearNet)\n",
        "aucpr_GearNet=(aucpr_GearNet).round(2)\n",
        "#######\n",
        "Yo_SVM=np.load(path+'All_SVM_Targets.npy')\n",
        "Z_SVM=np.load(path+'All_SVM_Scores.npy')\n",
        "####\n",
        "fpr_SVM, tpr_SVM, thresholds_SVM = roc_curve(Yo_SVM, Z_SVM)\n",
        "Auc_SVM = roc_auc_score(Yo_SVM, Z_SVM)\n",
        "Auc_SVM=(Auc_SVM).round(2)\n",
        "# calculate precision-recall curve\n",
        "precision_SVM, recall_SVM, thresholds = precision_recall_curve(Yo_SVM, Z_SVM)\n",
        "aucpr_SVM=auc(recall_SVM,precision_SVM)\n",
        "aucpr_SVM=(aucpr_SVM).round(2)\n",
        "#####Change this\n",
        "Yo_GNN=np.load(path+'All_GNN_Targets.npy')\n",
        "Z_GNN=np.load(path+'All_GNN_Scores.npy')\n",
        "##########\n",
        "fpr_GNN, tpr_GNN, thresholds_GNN = roc_curve(Yo_GNN, Z_GNN)\n",
        "Auc_GNN= roc_auc_score(Yo_GNN, Z_GNN)\n",
        "Auc_GNN=(Auc_GNN).round(2)\n",
        "# calculate precision-recall curve\n",
        "precision_GNN, recall_GNN, thresholds = precision_recall_curve(Yo_GNN, Z_GNN)\n",
        "aucpr_GNN=auc(recall_GNN,precision_GNN)\n",
        "aucpr_GNN=(aucpr_GNN).round(2)\n",
        "###### GNN LOCO average 0.8576 ± 0.0923 0.4366 ± 0.2003\n",
        "##### SVM LOCO average 0.7445 ± 0.1958 0.3312 ± 0.2017\n",
        "fig = plt.figure()\n",
        "Auc_GNN_std,PR_GNN_std,Auc_SVM_std,PR_SVM_std=0.0923,0.2003,0.1958,0.1958\n",
        "Auc_GearNet_std,PR_GearNet_std=0.1,0.14\n",
        "#text=\"There is an upcoming task in %d days at %d cluster!\" %a %cluster\n",
        "plt.plot(fpr_GNN,tpr_GNN,color='m',marker=',',markersize=2,label = ('GNN AUC-ROC : $ {} ± {}$').format(round(Auc_GNN,2), round(Auc_GNN_std,2)))\n",
        "plt.plot(fpr_SVM,tpr_SVM,color='b',marker=',',markersize=2,label=('SVM AUC-ROC : $ {} ± {}$').format(round(Auc_SVM,2), round(Auc_SVM_std,2)))\n",
        "plt.plot(fpr_GearNet,tpr_GearNet,color='k',marker='.', markersize=3,label=('GearNet AUC-ROC : $ {} ± {}$').format(round(Auc_GearNet,2),round(Auc_GearNet_std,2)))\n",
        "plt.title('AUCROC');plt.xlabel('FPR');plt.ylabel('TPR');plt.grid();plt.legend();plt.show();\n",
        "fig .savefig(path+\"Comaprison of AUCROC SVM and GNN-base model PPI Inhibitors Random and Binders combine Negative.pdf\", bbox_inches='tight')\n",
        "#########\n",
        "fig = plt.figure()\n",
        "plt.plot(recall_GNN,precision_GNN,color='m',marker=',',markersize=2,label=('GNN AUC-PR : $ {} ± {}$').format(round(aucpr_GNN,2), round(PR_GNN_std,2)))\n",
        "plt.plot(recall_SVM,precision_SVM,color='b',marker=',',markersize=2,label=('SVM AUC-PR: $ {} ± {}$').format(round(aucpr_SVM,2), round(PR_SVM_std,2)))\n",
        "plt.plot(recall_GearNet,precision_GearNet,color='k',marker='.', markersize=3,label=('GearNet AUC-PR: $ {} ± {}$').format(round(aucpr_GearNet,2), round(PR_GearNet_std,2)))\n",
        "plt.title('AUC-PR');plt.xlabel('Recall');plt.ylabel('Precision');plt.grid();plt.legend();plt.show();\n",
        "fig .savefig(path+\"Comaprison of AUC-PR SVM and GNN-base model PPI Inhibitors  Random and Binders combine.pdf\", bbox_inches='tight')"
      ],
      "metadata": {
        "id": "IlgI8EnYxPoS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}